{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "import random\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "\n",
    "# Variables for tuning\n",
    "N_EPISODES = 1000\n",
    "state_space = 112\n",
    "action_space = 7\n",
    "training_mode = True    # True if training phase, False if test phase\n",
    "\n",
    "class DQN:\n",
    "    def __init__(self, state_space, action_space):\n",
    "        self.state_space = state_space\n",
    "        self.action_space = action_space\n",
    "        self.learning_rate = 0.001\n",
    "        self.minibatch_size = 64\n",
    "        self.memory = []\n",
    "        self.gamma = 0.9                # discount rate\n",
    "        self.exploration_rate = 1      # exploration rate\n",
    "        self.epsilon_decay = 0.955\n",
    "        self.epsilon_min = 0.01\n",
    "        self.model = self.network()\n",
    "        self.memory_length = 3000\n",
    "\n",
    "    # Defining DQN.\n",
    "    # properties of DQN ------------------------------\n",
    "    def network(self, weights = None):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(output_dim=100, activation='tanh', input_dim=self.state_space))\n",
    "        model.add(Dense(output_dim=100, activation='tanh'))\n",
    "        model.add(Dense(output_dim=self.action_space, activation='softmax'))\n",
    "        opt = Adam(self.learning_rate)\n",
    "        model.compile(loss='mse', optimizer=opt)\n",
    "\n",
    "        if weights:\n",
    "            model.load_weights(weights)\n",
    "            print(\"weights loaded\")\n",
    "        return model\n",
    "\n",
    "    # Predict action based on current state\n",
    "    def do_action(self, state):\n",
    "        # TODO: convert from one-hot to continuous\n",
    "        action = np.zeros(7)\n",
    "        if np.random.rand() <= self.exploration_rate:\n",
    "            action[np.random.randint(0, 6)] = 1\n",
    "            arr = \"RANDOM\"\n",
    "            return action, arr\n",
    "        arr = self.model.predict(state.reshape(1,state_space))\n",
    "        action[np.argmax(arr[0])] = 1\n",
    "        arr = arr.reshape(7)\n",
    "        return action, arr\n",
    "\n",
    "    # Store info for replay memory\n",
    "    # store in global variable memory and returns it\n",
    "    def remember(self, state, action, next_state, reward, done):              # not consider: timeout, grass.\n",
    "        if((len(self.memory) + 1) > self.memory_length):\n",
    "            del self.memory[0]\n",
    "        self.memory.append([state, action, next_state, reward, done])\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        if len(self.memory) > self.minibatch_size:\n",
    "            minibatch = random.sample(self.memory, self.minibatch_size)\n",
    "        else:\n",
    "            minibatch = self.memory\n",
    "\n",
    "        for state, action, next_state, reward,  done in minibatch:\n",
    "            target = reward\n",
    "\n",
    "            if done != 5:\n",
    "                target = reward + self.gamma * np.amax(self.model.predict(next_state.reshape(1,state_space)))\n",
    "            target_f = self.model.predict(state.reshape(1,state_space))\n",
    "            target_f[0][np.argmax(action)] = target\n",
    "            self.model.fit(state.reshape(1,state_space), target_f, epochs= 1, verbose=0)\n",
    "\n",
    "        #if self.exploration_rate > self.epsilon_min:\n",
    "         #   self.exploration_rate *= self.epsilon_decay\n",
    "\n",
    "\n",
    "\n",
    "    def get_reward_perfect(self, done, state):\n",
    "        state_reward = state[:-2]\n",
    "        quadratic_deviation = state[-2]\n",
    "        state_reward = np.resize(state_reward,(10,11))\n",
    "        reward = 0\n",
    "\n",
    "        # LIVE REWARD ---------\n",
    "        for i in range(10):  # negative reward linear to occupancy along the cells. worst case: -27.5\n",
    "            crash_p_car = state_reward[i][0]           # check probability of car occupancy\n",
    "            crash_p_bike = state_reward[i][6]          # check probability of bike occupancy\n",
    "\n",
    "            reward += -(5 - 0.5*i) * crash_p_car       # check probability of car occupancy\n",
    "            reward += -(5 - 0.5*i) * crash_p_bike      # check probability of bike occupancy\n",
    "            reward += -quadratic_deviation             # check quadratic deviation, already normalized\n",
    "\n",
    "        # FINAL REWARD ---------\n",
    "        if done == 5:\n",
    "            reward += - 100  # if high occupancy probability\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def get_reward_exp1(self, done, state, row):\n",
    "        '''\n",
    "        Reward function designed by the expert\n",
    "        :param crash: true if ego crashed, false otherwise   ------------- done[2]\n",
    "        :param state: feature space wrt ego\n",
    "        :param row: list of vehicles and bikes with row wrt ego\n",
    "        :return: cumulative reward considering its various components based on the state\n",
    "        '''\n",
    "\n",
    "        arr_reward = np.zeros(5)\n",
    "        for i in range(3):\n",
    "            crash_p_car = state[i][0]  # check probability of occupancy in first 3 cells\n",
    "            if (crash_p_car > 0.5):\n",
    "                arr_reward[0] = 1\n",
    "                break\n",
    "            crash_p_bike = state[i][8]  # check probability of occupancy in first 3 cells\n",
    "            if (crash_p_bike > 0.5):\n",
    "                arr_reward[0] = 1\n",
    "                break\n",
    "\n",
    "        for i in range(3):\n",
    "            right_of_way_car = len(row[i][0])  # check if car objects had right of way in first 3 cells\n",
    "            right_of_way_bike = len(row[i][2])  # check if car objects had right of way in first 3 cells\n",
    "\n",
    "            if (right_of_way_car or right_of_way_bike):\n",
    "                arr_reward[1] = 1\n",
    "                break\n",
    "\n",
    "        for i in range(3):\n",
    "            is_crossing = state[i][17]\n",
    "            if (is_crossing):  # check if ego crashes in crossing path\n",
    "                arr_reward[2] = 1\n",
    "                break\n",
    "\n",
    "        for i in range(3):\n",
    "            crash_p_car = state[i][0]  # check probability of occupancy in first 3 cells\n",
    "            if (crash_p_car > 0.5):\n",
    "                arr_reward[3] = 1\n",
    "                break\n",
    "            crash_p_bike = state[i][8]  # check probability of occupancy in first 3 cells\n",
    "            if (crash_p_bike > 0.5):\n",
    "                arr_reward[3] = 1\n",
    "                break\n",
    "\n",
    "        for i in range(3):\n",
    "            right_of_way_car = len(row[i][0])  # check if car objects had right of way in first 3 cells\n",
    "            right_of_way_bike = len(row[i][2])  # check if car objects had right of way in first 3 cells\n",
    "\n",
    "            if (right_of_way_car or right_of_way_bike):\n",
    "                arr_reward[4] = 1\n",
    "                break\n",
    "        reward = 0\n",
    "        if done == 5:                  # crash with ego\n",
    "            if(arr_reward[0]):\n",
    "                reward = reward - 2  # if high occupancy probability\n",
    "            if (arr_reward[1]):\n",
    "                reward = reward - 4  # if actors had row\n",
    "            if (arr_reward[2]):\n",
    "                reward = reward - 5  # if ego in crossing path\n",
    "        else:\n",
    "            if (arr_reward[0]):\n",
    "                reward = reward + 2  # if high occupancy probability\n",
    "            if (arr_reward[1]):\n",
    "                reward = reward + 1  # if actors had row\n",
    "\n",
    "        return reward\n",
    "\n",
    "\n",
    "def discretize(element, n_bin, minimum, maximum):\n",
    "    try:\n",
    "        array = np.zeros(n_bin)\n",
    "        if element == maximum:\n",
    "            pos = n_bin - 1\n",
    "        else:\n",
    "            interval = (maximum - minimum) / n_bin\n",
    "            pos = math.floor(element / interval)\n",
    "        array[pos] = 1\n",
    "        return array\n",
    "    except IndexError:\n",
    "        print(\"Element outside the range\")\n",
    "\n",
    "# Main\n",
    "if __name__ == \"__main__\":\n",
    "    state_space = 5\n",
    "    action_space = 72\n",
    "    agent = DQN(state_space, action_space)\n",
    "    model = agent.network()\n",
    "    score = []\n",
    "    episode = []\n",
    "    episode_temp = 0\n",
    "    while True:\n",
    "        \n",
    "        # Get state from C++\n",
    "        f = open(\"state.txt\", \"r\")\n",
    "        lines = f.readlines()\n",
    "        if(len(lines)>1):\n",
    "            while lines[0] != \"1\\n\":\n",
    "                f = open(\"state.txt\", \"r\")\n",
    "                lines = f.readlines()\n",
    "            print(\"fino a qui tutto bene1\")\n",
    "            f.close()  \n",
    "            x, y, z, nx, ny, nz = lines[1].split(',')\n",
    "            f = open('state.txt', \"w\")\n",
    "            string = '0'\n",
    "            f.write(string)\n",
    "            f.close()\n",
    "            done = False\n",
    "        \n",
    "            while not done:\n",
    "                \n",
    "                # Predict action\n",
    "                action = agent.do_action(state)\n",
    "                \n",
    "                # Send action to C++\n",
    "                f = open(\"action.txt\", \"r\")\n",
    "                lines = f.readlines()\n",
    "                print(\"fino a qui tutto bene2\")\n",
    "                while lines[0] != \"1\\n\":\n",
    "                    f = open(\"python-write.txt\", \"r\")\n",
    "                    lines = f.readlines()\n",
    "                print(\"fino a qui tutto bene3\")\n",
    "                f.close()     \n",
    "                f = open('action.txt', \"w\")\n",
    "                f.write('0\\n')\n",
    "                f.write(action)\n",
    "                f.close()\n",
    "                \n",
    "                # Read SARS'A\n",
    "                f = open(\"sarsa.txt\", \"r\")\n",
    "                lines = f.readlines()\n",
    "                if(len(lines)>1):\n",
    "                    while lines[0] != \"1\\n\":\n",
    "                        f = open(\"sarsa.txt\", \"r\")\n",
    "                        lines = f.readlines()\n",
    "                    print(\"fino a qui tutto bene4\")\n",
    "                    f.close()  \n",
    "                    reward = lines[1]\n",
    "                    next_state = lines[2]                    \n",
    "                    f = open('sarsa.txt', \"w\")\n",
    "                    string = '0'\n",
    "                    f.write(string)\n",
    "                    f.close()\n",
    "                    \n",
    "                    remember()\n",
    "                    \n",
    "                    if reward >5:\n",
    "                        done = True\n",
    "\n",
    "                \n",
    "                \n",
    "        state = np.reshape(state, [1, state_space])\n",
    "        done = False\n",
    "        score_temp = 0\n",
    "        while not done:\n",
    "            env.render()\n",
    "            a = agent.do_action(state)\n",
    "            next_state, reward, done, _ = env.step(a)\n",
    "            #next_state = np.reshape(next_state, [0, state_space])\n",
    "            reward = reward if not done else -1\n",
    "            if training_mode:\n",
    "                agent.remember(state, a, next_state, reward, done)\n",
    "                agent.train()\n",
    "            state = next_state\n",
    "            score_temp += 1\n",
    "            episode_temp += 1\n",
    "\n",
    "        # PLOT OF THE SCORE\n",
    "        episode.append(episode_temp)\n",
    "        score.append(score_temp)\n",
    "        print(score_temp)\n",
    "        if(score_temp > 400):\n",
    "            sns_plot = sns.lineplot(episode, score)\n",
    "            sns_plot.set(xlabel='episode', ylabel='score')\n",
    "            fig = sns_plot.get_figure()\n",
    "            fig.savefig(\"output.png\")\n",
    "            break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
